{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation on Amazon SageMaker with Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "In many ways, recommender systems were a catalyst for the current popularity of machine learning.  One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature, while the million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "Recommender systems can utilize a multitude of data sources and ML algorithms, and most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework.  However, the core component is almost always a model which which predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users.\n",
    "\n",
    "Factorization Machines have shown themselves to be very effective at this task, and Amazon SageMaker includes an almost perfectly scalable implementation of a distributed Factorization Machine algorithm.  Today we'll use that to build a recommender system.  We'll start by bringing in and preparing our dataset, and then discuss the algorithm and training process, and finish by deploying to a real-time endpoint for generating predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Let's start by importing our necessary modules here, to get it out of the way.  Also, update the s3 bucket, prefix, or IAM role as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import json_deserializer\n",
    "\n",
    "import boto3, csv, io, json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults\n",
    "\n",
    "We will set a project name used to group Amazon SageMaker and Amazon S3 resources related to this specific notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'sagemaker-workshop-movie-recommender'\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.c5.18xlarge\"\n",
    "train_dataset_uri = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Identify SageMaker IAM service role\n",
    "\n",
    "SageMaker resources will need access to several resources to complete training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sagemaker_role():\n",
    "    try:\n",
    "        # if running within Amazon SageMaker Notebook Instance, get current execution role\n",
    "        role = get_execution_role()\n",
    "    except:\n",
    "        # otherwise, guess the role\n",
    "        result = %sx aws iam list-roles | grep -e 'role/service-role' | grep -i 'sagemaker' | tr -d ' '\n",
    "        role = result[0][7:-2]\n",
    "        print(\"Found potential SageMaker service role: {}\".format(role))\n",
    "    return role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found potential SageMaker service role: arn:aws:iam::306280812807:role/service-role/AmazonSageMaker-ExecutionRole-20180117T091311\n"
     ]
    }
   ],
   "source": [
    "role = get_sagemaker_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Identify SageMaker Container for specified region\n",
    "\n",
    "Set boto3 clients to use a valid SageMaker region and to select the proper container to use for training and endpoint deployment based on the specific algorithm to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting SageMaker region to: us-east-1\n",
      "Setting default S3 bucket to: sagemaker-workshop-movie-recommender-us-east-1\n"
     ]
    }
   ],
   "source": [
    "available_regions=['us-east-1','us-east-2','us-west-2','eu-west-1'] # available Amazon SageMaker regions\n",
    "region_name = boto3.Session().region_name\n",
    "if region_name not in available_regions:\n",
    "    region_name=available_regions[0]\n",
    "print('Setting SageMaker region to: {}'.format(region_name))\n",
    "default_bucket = '{}-{}'.format(project_name, region_name)\n",
    "print('Setting default S3 bucket to: {}'.format(default_bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Protocol Buffer formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'protobuf_5'\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Location': '/sagemaker-workshop-movie-recommender-us-east-1',\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',\n",
       "   'date': 'Thu, 03 May 2018 12:24:16 GMT',\n",
       "   'location': '/sagemaker-workshop-movie-recommender-us-east-1',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'dGLcTNEL1k7wO6+mRkBtE5kHZ5OfVuBmejhKtKiJiUxVHBFU6ALwvVbyfsM/S1v4bZh8SVofb2c=',\n",
       "   'x-amz-request-id': 'B05E272E28C91624'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HostId': 'dGLcTNEL1k7wO6+mRkBtE5kHZ5OfVuBmejhKtKiJiUxVHBFU6ALwvVbyfsM/S1v4bZh8SVofb2c=',\n",
       "  'RequestId': 'B05E272E28C91624',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: If not exists, create S3 bucket\n",
    "s3 = boto3.client(\"s3\", region_name=region_name)\n",
    "## check if bucket exists\n",
    "## if not...\n",
    "s3.create_bucket(Bucket=default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and look at our data\n",
    "Let's get our data. Factorization Machines work well as recommendation solutions, such as movie recommendations! Also, FM's do great on lots of data, so let's get something heavy. Maybe the MovieLens dataset, with 20 million ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-05-05 04:43:36--  http://files.grouplens.org/datasets/movielens/ml-latest.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.34.235\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.34.235|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248434223 (237M) [application/zip]\n",
      "Saving to: ‘/tmp/dataset.zip’\n",
      "\n",
      "/tmp/dataset.zip    100%[===================>] 236.92M  56.0MB/s    in 4.4s    \n",
      "\n",
      "2018-05-05 04:43:40 (54.0 MB/s) - ‘/tmp/dataset.zip’ saved [248434223/248434223]\n",
      "\n",
      "Archive:  /tmp/dataset.zip\n",
      "  inflating: /tmp/dataset/genome-scores.csv  \n",
      "  inflating: /tmp/dataset/genome-tags.csv  \n",
      "  inflating: /tmp/dataset/links.csv  \n",
      "  inflating: /tmp/dataset/movies.csv  \n",
      "  inflating: /tmp/dataset/ratings.csv  \n",
      "  inflating: /tmp/dataset/README.txt  \n",
      "  inflating: /tmp/dataset/tags.csv   \n"
     ]
    }
   ],
   "source": [
    "!rm -f dataset.zip\n",
    "!wget -O /tmp/dataset.zip $train_dataset_uri\n",
    "!unzip -j -o /tmp/dataset.zip -d /tmp/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top of this file, shall we? This should show us the range of user ids, which we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/dataset\n",
      "userId,movieId,rating,timestamp\n",
      "1,110,1.0,1425941529\n",
      "1,147,4.5,1425942435\n",
      "1,858,5.0,1425941523\n",
      "1,1221,5.0,1425941546\n",
      "1,1246,5.0,1425941556\n",
      "1,1968,4.0,1425942148\n",
      "1,2762,4.5,1425941300\n",
      "1,2918,5.0,1425941593\n",
      "1,2959,4.0,1425941601\n",
      "51433,172,5.0,1394863719\n",
      "51433,173,5.0,1394863483\n",
      "51433,208,5.0,1394863612\n",
      "51433,333,5.0,1394863271\n",
      "51433,442,5.0,1394863961\n",
      "51433,541,5.0,1394863673\n",
      "51433,661,5.0,1394863273\n",
      "51433,783,4.0,1394863276\n",
      "51433,1035,0.5,1394863180\n",
      "51433,1320,5.0,1394863183\n",
      "Users: 51433\n"
     ]
    }
   ],
   "source": [
    "%cd /tmp/dataset\n",
    "!head -5000000 ratings.csv > ratings.trunc.csv\n",
    "!mv ratings.trunc.csv ratings.csv\n",
    "!head -10 ratings.csv\n",
    "!tail -10 ratings.csv\n",
    "result=!tail -1 ratings.csv | cut -d',' -f1\n",
    "user_count=int(result[0])\n",
    "print(\"Users: {}\".format(user_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare our data\n",
    "\n",
    "We remove the header, then shuffle the data before splitting it into training and prediction segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3999992\n",
      "50100,34437,4.5,1213523382\n",
      "25930,6281,3.5,1197551064\n",
      "1757,282,3.0,953069447\n",
      "2022,5452,3.5,1345325819\n",
      "30432,555,2.0,832423645\n",
      "Validation set: 499999\n",
      "13222,2761,4.0,1111442322\n",
      "5953,1246,5.0,1432706322\n",
      "32984,46083,2.5,1282142417\n",
      "24528,1597,2.0,1217279586\n",
      "37791,58494,3.5,1219669123\n",
      "Prediction set: 499999\n",
      "48309,2402,0.5,1172106649\n",
      "31104,45672,2.5,1466591524\n",
      "2359,2496,3.0,1203249423\n",
      "36379,223,5.0,1003617270\n",
      "7386,7802,4.0,1226628462\n"
     ]
    }
   ],
   "source": [
    "!tail -n +2 ratings.csv | shuf -o ratings.shuffled\n",
    "\n",
    "result=!wc -l ratings.shuffled | cut -d' ' -f1\n",
    "lines_total=result[0]\n",
    "lines_split=int(int(lines_total)/10)\n",
    "!split --lines $lines_split ratings.shuffled\n",
    "\n",
    "!cat xa{a..h} > ratings.shuffled.training\n",
    "!cat xai > ratings.shuffled.validation\n",
    "!cat xaj > ratings.shuffled.prediction\n",
    "\n",
    "result=!wc -l ratings.shuffled.training | cut -d' ' -f1\n",
    "lines_training=int(result[0])\n",
    "print(\"Training set: {}\".format(lines_training))\n",
    "!head -n5 ratings.shuffled.training\n",
    "\n",
    "result=!wc -l ratings.shuffled.validation | cut -d' ' -f1      \n",
    "lines_validation=int(result[0])\n",
    "print(\"Validation set: {}\".format(lines_validation))  \n",
    "!head -n5 ratings.shuffled.validation\n",
    "      \n",
    "result=!wc -l ratings.shuffled.prediction | cut -d' ' -f1\n",
    "lines_prediction=int(result[0])\n",
    "print(\"Prediction set: {}\".format(lines_prediction))\n",
    "!head -n5 ratings.shuffled.prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training set and test set\n",
    "\n",
    "When using Factorization Machines for building a recommender system, the algorithm expects the data to look something like:\n",
    "\n",
    "|Rating|User1|User2|...|UserN|Movie1|Movie2|Movie3|...|MovieM|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|4|1|0|...|0|1|0|0|...|0|\n",
    "|5|1|0|...|0|0|1|0|...|0|\n",
    "|3|0|1|...|0|1|0|0|...|0|\n",
    "|4|0|1|...|0|0|0|1|...|0|\n",
    "\n",
    "- Our target column is the rating for that user movie combination.  This could be the number of stars given, or could be a binarized version (movies with ratings 4 and above are set to `1`, otherwise `0`).  \n",
    "- We have a set of `N` features which are a one-hot encoding of user.  They only take a value of `1` to indicate observations which belong to that customer.\n",
    "- We also have a set of `M` features which are a one-hot encoding of movie.\n",
    "\n",
    "Sagemaker's Factorization Machines can handle sparse data, meaning we only store the non-zero entries, but to contruct the sparse feature matrices we need to figure out the size of the training and testing sets. We also normalized the movie ids here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedMovieIdDict = {}\n",
    "normalizedIdMovies = 0\n",
    "\n",
    "with open('ratings.shuffled','r') as f:\n",
    "    samples = csv.reader(f,delimiter=',')\n",
    "    for userId,movieId,rating,timestamp in samples:\n",
    "        if movieId not in normalizedMovieIdDict:\n",
    "            normalizedMovieIdDict[movieId] = normalizedIdMovies\n",
    "            normalizedIdMovies = normalizedIdMovies + 1\n",
    "\n",
    "\n",
    "nbUsers = user_count # Our max user ID in the tail of the user sorted file\n",
    "nbMovies = normalizedIdMovies\n",
    "\n",
    "nbFeatures = nbUsers+nbMovies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read our training and test datsets in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(filename, total_lines, columns):\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    \n",
    "    X = lil_matrix((total_lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line = 0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter=',')\n",
    "        for userId,movieId,rating,timestamp in samples:\n",
    "            X[line,int(userId)-1] = 1\n",
    "            normalizedMovieId = normalizedMovieIdDict[movieId]\n",
    "            X[line,int(nbUsers)+int(normalizedMovieId)-1] = 1\n",
    "            if float(rating) >= 4:\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            line = line+1\n",
    "            \n",
    "    Y = np.array(Y).astype('float32') \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 876 ms, total: 1min 17s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train, Y_train = loadDataset('ratings.shuffled.training', lines_training, nbFeatures)\n",
    "X_test, Y_test = loadDataset('ratings.shuffled.validation', lines_validation, nbFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to protobuf and save to S3\n",
    "We train our models from data in S3; writing a helper function here to help us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-workshop-movie-recommender-us-east-1/protobuf_5/train/train.protobuf\n",
      "s3://sagemaker-workshop-movie-recommender-us-east-1/protobuf_5/test/test.protobuf\n",
      "CPU times: user 9min 17s, sys: 6.79 s, total: 9min 24s\n",
      "Wall time: 9min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_data_location = writeDatasetToProtobuf(X_train, Y_train, default_bucket, train_prefix, train_key)    \n",
    "test_data_location  = writeDatasetToProtobuf(X_test, Y_test, default_bucket, test_prefix, test_key)\n",
    "\n",
    "train_data_location = \"s3://{}/{}/train/{}\".format(default_bucket, prefix, train_key)  \n",
    "test_data_location  = \"s3://{}/{}/test/{}\".format(default_bucket, prefix, test_key)\n",
    "  \n",
    "print(train_data_location)\n",
    "print(test_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Factorization Machine training job with validation\n",
    "Sagemaker algorithms are stored as Docker containers in ECS, and we need the URI of the containers. This will be region specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_containers = {\n",
    "    'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/factorization-machines:latest',\n",
    "    'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest',\n",
    "    'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:latest',\n",
    "    'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/factorization-machines:latest'}\n",
    "\n",
    "training_image = fm_containers[region_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time! Some things to note here that might not be obvious:\n",
    "<ul>\n",
    "    <li>You can specify the hardware you want to use; Sagemaker will create your cluster and deploy your image</li>\n",
    "    <li>You can optionally proviode a test dataset as well, which we have done here</li>\n",
    "    <li>The metrics will be printed out after each epoch and mini batch</li>\n",
    "</ul>\n",
    "\n",
    "In order to train our Factorization Machine, we'll need to specify some hyperparameters.  First, let's think about what a Factorization Machine does:  \n",
    "\n",
    "$$\\hat{r} = w_0 + \\sum_{i} {w_i x_i} + \\sum_{i} {\\sum_{j > i} {\\langle v_i, v_j \\rangle x_i x_j}}$$\n",
    "\n",
    "where $\\hat{r}$ is the movie rating, $x_i$ are the one-hot encoded user and movie indicators, $w_i$ are linear weights, and the second term represents the pairwise feature interactions as a reduced dimension factorization.  This reduction in dimensionality allows us to find a smaller, more generalizable representation of information in the large sparse input dataset.  Key hyperparameters are therefore:\n",
    "\n",
    "- `feature_dim`: How big our initial movie \n",
    "- `num_factors`: How large our reduced dimensionality representation of interactions should be\n",
    "\n",
    "We also specify hyperparameters like `mini_batch_size` and `epochs` which can be tuned for improved performance.\n",
    "\n",
    "Hit \"play\" and sit back to let Sagemaker do all the work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: factorization-machines-2018-05-03-12-35-11-657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:50 INFO 140437905205056] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'_speedometer_period': u'500', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:50 INFO 140437905205056] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'3', u'feature_dim': u'86151', u'mini_batch_size': u'1000', u'predictor_type': u'binary_classifier', u'num_factors': u'64'}\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:50 INFO 140437905205056] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'3', u'feature_dim': u'86151', u'num_factors': u'64', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'_speedometer_period': u'500', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'predictor_type': u'binary_classifier', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:50 WARNING 140437905205056] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:50 INFO 140437905205056] Using default worker.\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:50 INFO 140437905205056] nvidia-smi took: 0.0262939929962 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:50 INFO 140437905205056] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:50 INFO 140437905205056] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:50 INFO 140437905205056] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 37.1861457824707, \"sum\": 37.1861457824707, \"min\": 37.1861457824707}}, \"EndTime\": 1525351130.713932, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1525351130.64976}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1525351130.714089, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1525351130.714059}\n",
      "\u001b[0m\n",
      "\u001b[31m[12:38:50] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.200160.0/RHEL5_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[31m[12:38:50] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.200160.0/RHEL5_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:53 INFO 140437905205056] Epoch[1] Batch [500]#011Speed: 195065.25 samples/sec#011binary_classification_accuracy=0.606190#011binary_classification_cross_entropy=0.682682#011binary_f_1.000=0.583900\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:56 INFO 140437905205056] Epoch[1] Batch [1000]#011Speed: 184638.52 samples/sec#011binary_classification_accuracy=0.636873#011binary_classification_cross_entropy=0.674465#011binary_f_1.000=0.615398\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:38:58 INFO 140437905205056] Epoch[1] Batch [1500]#011Speed: 174874.22 samples/sec#011binary_classification_accuracy=0.652065#011binary_classification_cross_entropy=0.668385#011binary_f_1.000=0.630432\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:01 INFO 140437905205056] Epoch[1] Batch [2000]#011Speed: 174055.06 samples/sec#011binary_classification_accuracy=0.662281#011binary_classification_cross_entropy=0.663752#011binary_f_1.000=0.643387\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:04 INFO 140437905205056] Epoch[1] Batch [2500]#011Speed: 183914.40 samples/sec#011binary_classification_accuracy=0.669301#011binary_classification_cross_entropy=0.659842#011binary_f_1.000=0.651830\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:07 INFO 140437905205056] Epoch[1] Batch [3000]#011Speed: 184403.51 samples/sec#011binary_classification_accuracy=0.674798#011binary_classification_cross_entropy=0.656406#011binary_f_1.000=0.658745\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:10 INFO 140437905205056] Epoch[1] Batch [3500]#011Speed: 178893.37 samples/sec#011binary_classification_accuracy=0.679028#011binary_classification_cross_entropy=0.653349#011binary_f_1.000=0.663327\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}, \"update.time\": {\"count\": 1, \"max\": 22083.3420753479, \"sum\": 22083.3420753479, \"min\": 22083.3420753479}}, \"EndTime\": 1525351152.797614, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1525351130.714011}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:12 INFO 140437905205056] #progress_metric: Completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4000, \"sum\": 4000.0, \"min\": 4000}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4000, \"sum\": 4000.0, \"min\": 4000}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 3999992, \"sum\": 3999992.0, \"min\": 3999992}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4001, \"sum\": 4001.0, \"min\": 4001}, \"Total Records Seen\": {\"count\": 1, \"max\": 4000992, \"sum\": 4000992.0, \"min\": 4000992}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 3999992, \"sum\": 3999992.0, \"min\": 3999992}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1525351152.798216, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1525351152.79818}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:12 INFO 140437905205056] #throughput_metric: train throughput in records/second: 181126.082037\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:15 INFO 140437905205056] Epoch[1] Batch [500]#011Speed: 183212.81 samples/sec#011binary_classification_accuracy=0.715555#011binary_classification_cross_entropy=0.626186#011binary_f_1.000=0.711903\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:18 INFO 140437905205056] Epoch[1] Batch [1000]#011Speed: 182285.70 samples/sec#011binary_classification_accuracy=0.716189#011binary_classification_cross_entropy=0.624268#011binary_f_1.000=0.711372\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:21 INFO 140437905205056] Epoch[1] Batch [1500]#011Speed: 181700.44 samples/sec#011binary_classification_accuracy=0.717578#011binary_classification_cross_entropy=0.622111#011binary_f_1.000=0.711522\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:23 INFO 140437905205056] Epoch[1] Batch [2000]#011Speed: 182742.26 samples/sec#011binary_classification_accuracy=0.718520#011binary_classification_cross_entropy=0.620428#011binary_f_1.000=0.712778\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[05/03/2018 12:39:26 INFO 140437905205056] Epoch[1] Batch [2500]#011Speed: 180327.76 samples/sec#011binary_classification_accuracy=0.719085#011binary_classification_cross_entropy=0.618783#011binary_f_1.000=0.713168\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:29 INFO 140437905205056] Epoch[1] Batch [3000]#011Speed: 182907.59 samples/sec#011binary_classification_accuracy=0.719773#011binary_classification_cross_entropy=0.617214#011binary_f_1.000=0.713889\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:32 INFO 140437905205056] Epoch[1] Batch [3500]#011Speed: 181952.50 samples/sec#011binary_classification_accuracy=0.720278#011binary_classification_cross_entropy=0.615718#011binary_f_1.000=0.713891\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21953.569889068604, \"sum\": 21953.569889068604, \"min\": 21953.569889068604}}, \"EndTime\": 1525351174.752013, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1525351152.797685}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:34 INFO 140437905205056] #progress_metric: Completed 66 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4000, \"sum\": 4000.0, \"min\": 4000}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4000, \"sum\": 4000.0, \"min\": 4000}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 3999992, \"sum\": 3999992.0, \"min\": 3999992}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8001, \"sum\": 8001.0, \"min\": 8001}, \"Total Records Seen\": {\"count\": 1, \"max\": 8000984, \"sum\": 8000984.0, \"min\": 8000984}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 3999992, \"sum\": 3999992.0, \"min\": 3999992}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1525351174.752329, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1525351174.752298}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:34 INFO 140437905205056] #throughput_metric: train throughput in records/second: 182199.028609\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:37 INFO 140437905205056] Epoch[1] Batch [500]#011Speed: 180773.96 samples/sec#011binary_classification_accuracy=0.727551#011binary_classification_cross_entropy=0.601365#011binary_f_1.000=0.724795\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:40 INFO 140437905205056] Epoch[1] Batch [1000]#011Speed: 182629.73 samples/sec#011binary_classification_accuracy=0.727723#011binary_classification_cross_entropy=0.600110#011binary_f_1.000=0.723990\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:42 INFO 140437905205056] Epoch[1] Batch [1500]#011Speed: 182998.96 samples/sec#011binary_classification_accuracy=0.728566#011binary_classification_cross_entropy=0.598464#011binary_f_1.000=0.723722\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:45 INFO 140437905205056] Epoch[1] Batch [2000]#011Speed: 180386.32 samples/sec#011binary_classification_accuracy=0.728987#011binary_classification_cross_entropy=0.597401#011binary_f_1.000=0.724283\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:48 INFO 140437905205056] Epoch[1] Batch [2500]#011Speed: 178697.54 samples/sec#011binary_classification_accuracy=0.729030#011binary_classification_cross_entropy=0.596330#011binary_f_1.000=0.724115\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:51 INFO 140437905205056] Epoch[1] Batch [3000]#011Speed: 184107.68 samples/sec#011binary_classification_accuracy=0.729336#011binary_classification_cross_entropy=0.595326#011binary_f_1.000=0.724393\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:54 INFO 140437905205056] Epoch[1] Batch [3500]#011Speed: 181095.50 samples/sec#011binary_classification_accuracy=0.729441#011binary_classification_cross_entropy=0.594356#011binary_f_1.000=0.724020\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22041.435956954956, \"sum\": 22041.435956954956, \"min\": 22041.435956954956}}, \"EndTime\": 1525351196.794, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1525351174.752079}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:56 INFO 140437905205056] #progress_metric: Completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 4000, \"sum\": 4000.0, \"min\": 4000}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 4000, \"sum\": 4000.0, \"min\": 4000}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 3999992, \"sum\": 3999992.0, \"min\": 3999992}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12001, \"sum\": 12001.0, \"min\": 12001}, \"Total Records Seen\": {\"count\": 1, \"max\": 12000976, \"sum\": 12000976.0, \"min\": 12000976}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 3999992, \"sum\": 3999992.0, \"min\": 3999992}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1525351196.794315, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1525351196.794276}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:56 INFO 140437905205056] #throughput_metric: train throughput in records/second: 181472.805303\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:56 WARNING 140437905205056] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:56 INFO 140437905205056] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 11.358976364135742, \"sum\": 11.358976364135742, \"min\": 11.358976364135742}}, \"EndTime\": 1525351196.805941, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1525351196.794058}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:56 INFO 140437905205056] Saved checkpoint to \"/tmp/tmpmwhUSF/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:58 INFO 140437905205056] #test_score (algo-1) : binary_classification_accuracy\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:58 INFO 140437905205056] #test_score (algo-1) : 0.725115450231\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:58 INFO 140437905205056] #test_score (algo-1) : binary_classification_cross_entropy\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:58 INFO 140437905205056] #test_score (algo-1) : 0.59097773469\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:58 INFO 140437905205056] #test_score (algo-1) : binary_f_1.000\u001b[0m\n",
      "\u001b[31m[05/03/2018 12:39:58 INFO 140437905205056] #test_score (algo-1) : 0.732316681274\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 500, \"sum\": 500.0, \"min\": 500}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 500, \"sum\": 500.0, \"min\": 500}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 499999, \"sum\": 499999.0, \"min\": 499999}, \"Total Batches Seen\": {\"count\": 1, \"max\": 500, \"sum\": 500.0, \"min\": 500}, \"Total Records Seen\": {\"count\": 1, \"max\": 499999, \"sum\": 499999.0, \"min\": 499999}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 499999, \"sum\": 499999.0, \"min\": 499999}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1525351198.735369, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1525351198.735332}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 68119.64201927185, \"sum\": 68119.64201927185, \"min\": 68119.64201927185}, \"setuptime\": {\"count\": 1, \"max\": 29.002904891967773, \"sum\": 29.002904891967773, \"min\": 29.002904891967773}}, \"EndTime\": 1525351198.73697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1525351196.80604}\n",
      "\u001b[0m\n",
      "===== Job Complete =====\n",
      "Billable seconds: 166\n",
      "CPU times: user 364 ms, sys: 12 ms, total: 376 ms\n",
      "Wall time: 5min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fm_trained_model_location  = 's3://{}/{}/{}/output'.format(default_bucket, prefix, \"fm\")\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    train_instance_count=train_instance_count, \n",
    "    train_instance_type=train_instance_type,\n",
    "    output_path=fm_trained_model_location,\n",
    "    sagemaker_session=sagemaker.Session())\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=nbFeatures,\n",
    "    predictor_type='binary_classifier',\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=64,\n",
    "    epochs=3)\n",
    "\n",
    "fm.fit({'train': train_data_location, 'test': test_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model\n",
    "Now our trained model is stored, and you can actually see this in the Sagmaker console. We can go ahead and deploy this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: factorization-machines-2018-05-03-12-56-01-209\n",
      "INFO:sagemaker:Creating endpoint with name factorization-machines-2018-05-03-12-35-11-657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------!CPU times: user 324 ms, sys: 8 ms, total: 332 ms\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fm_predictor = fm.deploy(instance_type='ml.c5.2xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make requests to our endpoint for prediction. Our data is really sparse (only 2 values in each vector row of 165237), and it doesn't make sense to send matrices that are mostly empty. To solve this problem, we are going to go ahead and create a protobuf serializer to send content in a sparse format is supported by sagemaker.\n",
    "\n",
    "You can see more formats here: https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_protobuf_serializer(data):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, data)\n",
    "    buf.seek(0)\n",
    "    return buf\n",
    "\n",
    "fm_predictor.content_type = 'application/x-recordio-protobuf'\n",
    "fm_predictor.serializer = sparse_protobuf_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions\n",
    "We made a dataset to run prediction earlier in this notebook. Let's go ahead and load that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred, Y_pred = loadDataset('ratings.shuffled.prediction', lines_prediction , nbFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab single row and run prediction on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'score': 0.4593372642993927, 'predicted_label': 0.0}]}\n",
      "TrueLabel:0.0 PredictedLabel:0 Score:0.4593372642993927\n"
     ]
    }
   ],
   "source": [
    "fm_result = fm_predictor.predict(X_pred[:1])\n",
    "print(fm_result)\n",
    "true_labels = Y_pred[:1]\n",
    "\n",
    "for l,r in zip(true_labels, fm_result['predictions']):\n",
    "    print(\"TrueLabel:{} PredictedLabel:{} Score:{}\".format(l, int(r['predicted_label']), r['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of training job's accuracy\n",
    "Now that we've sanity checked that, let's just run inference on the entire prediction dataset of 2 million rows. Our feature dimension is wide but our data is mostly sparse, so we should be able to do relatively big batches of inferences. (This will take a couple of minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.2 s, sys: 596 ms, total: 45.8 s\n",
      "Wall time: 49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = lines_prediction\n",
    "predictions = []\n",
    "\n",
    "batches = int(X_pred.shape[0]/batch_size)\n",
    "\n",
    "for i in range(batches):\n",
    "    X_sample = X_pred[ (i*batch_size) : ((i+1)*batch_size) ]\n",
    "    result = fm_predictor.predict(X_sample)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'score': 0.7121307253837585, 'predicted_label': 1.0}, {'score': 0.18819570541381836, 'predicted_label': 0.0}, {'score': 0.5745882987976074, 'predicted_label': 1.0}]}\n",
      "[1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "fm_result = fm_predictor.predict(X_pred[1000:1003])\n",
    "print(fm_result)\n",
    "print(Y_pred[1000:1003])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what percentage we recommended correctly. This should be roughly equivalent to our testing metric that was reported during training, but provides independent validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]\n",
    "np.sum(Y_pred == predictions) / len(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a substantial improvement over a naive baseline ... add more here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up, and finish!\n",
    "And we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: factorization-machines-2018-05-03-12-35-11-657\n"
     ]
    }
   ],
   "source": [
    "#fm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
