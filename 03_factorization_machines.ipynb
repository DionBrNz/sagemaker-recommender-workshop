{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation on Amazon SageMaker with Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "In many ways, recommender systems were a catalyst for the current popularity of machine learning.  One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature, while the million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "Recommender systems can utilize a multitude of data sources and ML algorithms, and most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework.  However, the core component is almost always a model which which predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users.\n",
    "\n",
    "Factorization Machines have shown themselves to be very effective at this task, and Amazon SageMaker includes an almost perfectly scalable implementation of a distributed Factorization Machine algorithm.  Today we'll use that to build a recommender system.  We'll start by bringing in and preparing our dataset, and then discuss the algorithm and training process, and finish by deploying to a real-time endpoint for generating predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Let's start by importing our necessary modules here, to get it out of the way.  Also, update the s3 bucket, prefix, or IAM role as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import json_deserializer\n",
    "\n",
    "import boto3, csv, io, json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults\n",
    "\n",
    "We will set a project name used to group Amazon SageMaker and Amazon S3 resources related to this specific notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'sagemaker-workshop-movie-recommender'\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.c5.18xlarge\"\n",
    "train_dataset_uri = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Identify SageMaker IAM service role\n",
    "\n",
    "SageMaker resources will need access to several resources to complete training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sagemaker_role():\n",
    "    try:\n",
    "        # if running within Amazon SageMaker Notebook Instance, get current execution role\n",
    "        role = get_execution_role()\n",
    "    except:\n",
    "        # otherwise, guess the role\n",
    "        result = %sx aws iam list-roles | grep -e 'role/service-role' | grep -i 'sagemaker' | tr -d ' '\n",
    "        role = result[0][7:-2]\n",
    "        print(\"Found potential SageMaker service role: {}\".format(role))\n",
    "    return role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_sagemaker_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Identify SageMaker Container for specified region\n",
    "\n",
    "Set boto3 clients to use a valid SageMaker region and to select the proper container to use for training and endpoint deployment based on the specific algorithm to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_regions=['us-east-1','us-east-2','us-west-2','eu-west-1'] # available Amazon SageMaker regions\n",
    "region_name = boto3.Session().region_name\n",
    "if region_name not in available_regions:\n",
    "    region_name=available_regions[0]\n",
    "print('Setting SageMaker region to: {}'.format(region_name))\n",
    "default_bucket = '{}-{}'.format(project_name, region_name)\n",
    "print('Setting default S3 bucket to: {}'.format(default_bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Protocol Buffer formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'protobuf_5'\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: If not exists, create S3 bucket\n",
    "s3 = boto3.client(\"s3\", region_name=region_name)\n",
    "## check if bucket exists\n",
    "## if not...\n",
    "s3.create_bucket(Bucket=default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and look at our data\n",
    "Let's get our data. Factorization Machines work well as recommendation solutions, such as movie recommendations! Also, FM's do great on lots of data, so let's get something heavy. Maybe the MovieLens dataset, with 20 million ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f /tmp/dataset.zip\n",
    "!wget -O /tmp/dataset.zip $train_dataset_uri\n",
    "!unzip -j -o /tmp/dataset.zip -d /tmp/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top of this file, shall we? This should show us the range of user ids, which we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /tmp/dataset\n",
    "x -5000000 ratings.csv > ratings.trunc.csv\n",
    "!mv ratings.trunc.csv ratings.csv\n",
    "!head -10 ratings.csv\n",
    "!tail -10 ratings.csv\n",
    "result=!tail -1 ratings.csv | cut -d',' -f1\n",
    "user_count=int(result[0])\n",
    "print(\"Users: {}\".format(user_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare our data\n",
    "\n",
    "We remove the header, then shuffle the data before splitting it into training and prediction segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n +2 ratings.csv | shuf -o ratings.shuffled\n",
    "\n",
    "result=!wc -l ratings.shuffled | cut -d' ' -f1\n",
    "lines_total=result[0]\n",
    "lines_split=int(int(lines_total)/10)\n",
    "!split --lines $lines_split ratings.shuffled\n",
    "\n",
    "!cat xa{a..h} > ratings.shuffled.training\n",
    "!cat xai > ratings.shuffled.validation\n",
    "!cat xaj > ratings.shuffled.prediction\n",
    "\n",
    "result=!wc -l ratings.shuffled.training | cut -d' ' -f1\n",
    "lines_training=int(result[0])\n",
    "print(\"Training set: {}\".format(lines_training))\n",
    "!head -n5 ratings.shuffled.training\n",
    "\n",
    "result=!wc -l ratings.shuffled.validation | cut -d' ' -f1      \n",
    "lines_validation=int(result[0])\n",
    "print(\"Validation set: {}\".format(lines_validation))  \n",
    "!head -n5 ratings.shuffled.validation\n",
    "      \n",
    "result=!wc -l ratings.shuffled.prediction | cut -d' ' -f1\n",
    "lines_prediction=int(result[0])\n",
    "print(\"Prediction set: {}\".format(lines_prediction))\n",
    "!head -n5 ratings.shuffled.prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training set and test set\n",
    "\n",
    "When using Factorization Machines for building a recommender system, the algorithm expects the data to look something like:\n",
    "\n",
    "|Rating|User1|User2|...|UserN|Movie1|Movie2|Movie3|...|MovieM|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|4|1|0|...|0|1|0|0|...|0|\n",
    "|5|1|0|...|0|0|1|0|...|0|\n",
    "|3|0|1|...|0|1|0|0|...|0|\n",
    "|4|0|1|...|0|0|0|1|...|0|\n",
    "\n",
    "- Our target column is the rating for that user movie combination.  This could be the number of stars given, or could be a binarized version (movies with ratings 4 and above are set to `1`, otherwise `0`).  \n",
    "- We have a set of `N` features which are a one-hot encoding of user.  They only take a value of `1` to indicate observations which belong to that customer.\n",
    "- We also have a set of `M` features which are a one-hot encoding of movie.\n",
    "\n",
    "Sagemaker's Factorization Machines can handle sparse data, meaning we only store the non-zero entries, but to contruct the sparse feature matrices we need to figure out the size of the training and testing sets. We also normalized the movie ids here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedMovieIdDict = {}\n",
    "normalizedIdMovies = 0\n",
    "\n",
    "with open('ratings.shuffled','r') as f:\n",
    "    samples = csv.reader(f,delimiter=',')\n",
    "    for userId,movieId,rating,timestamp in samples:\n",
    "        if movieId not in normalizedMovieIdDict:\n",
    "            normalizedMovieIdDict[movieId] = normalizedIdMovies\n",
    "            normalizedIdMovies = normalizedIdMovies + 1\n",
    "\n",
    "\n",
    "nbUsers = user_count # Our max user ID in the tail of the user sorted file\n",
    "nbMovies = normalizedIdMovies\n",
    "\n",
    "nbFeatures = nbUsers+nbMovies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read our training and test datsets in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(filename, total_lines, columns):\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    \n",
    "    X = lil_matrix((total_lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line = 0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter=',')\n",
    "        for userId,movieId,rating,timestamp in samples:\n",
    "            X[line,int(userId)-1] = 1\n",
    "            normalizedMovieId = normalizedMovieIdDict[movieId]\n",
    "            X[line,int(nbUsers)+int(normalizedMovieId)-1] = 1\n",
    "            if float(rating) >= 4:\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            line = line+1\n",
    "            \n",
    "    Y = np.array(Y).astype('float32') \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train, Y_train = loadDataset('ratings.shuffled.training', lines_training, nbFeatures)\n",
    "X_test, Y_test = loadDataset('ratings.shuffled.validation', lines_validation, nbFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to protobuf and save to S3\n",
    "We train our models from data in S3; writing a helper function here to help us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_data_location = writeDatasetToProtobuf(X_train, Y_train, default_bucket, train_prefix, train_key)    \n",
    "test_data_location  = writeDatasetToProtobuf(X_test, Y_test, default_bucket, test_prefix, test_key)\n",
    "\n",
    "train_data_location = \"s3://{}/{}/train/{}\".format(default_bucket, prefix, train_key)  \n",
    "test_data_location  = \"s3://{}/{}/test/{}\".format(default_bucket, prefix, test_key)\n",
    "  \n",
    "print(train_data_location)\n",
    "print(test_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Factorization Machine training job with validation\n",
    "Sagemaker algorithms are stored as Docker containers in ECS, and we need the URI of the containers. This will be region specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_containers = {\n",
    "    'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/factorization-machines:latest',\n",
    "    'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest',\n",
    "    'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:latest',\n",
    "    'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/factorization-machines:latest'}\n",
    "\n",
    "training_image = fm_containers[region_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time! Some things to note here that might not be obvious:\n",
    "<ul>\n",
    "    <li>You can specify the hardware you want to use; Sagemaker will create your cluster and deploy your image</li>\n",
    "    <li>You can optionally proviode a test dataset as well, which we have done here</li>\n",
    "    <li>The metrics will be printed out after each epoch and mini batch</li>\n",
    "</ul>\n",
    "\n",
    "In order to train our Factorization Machine, we'll need to specify some hyperparameters.  First, let's think about what a Factorization Machine does:  \n",
    "\n",
    "$$\\hat{r} = w_0 + \\sum_{i} {w_i x_i} + \\sum_{i} {\\sum_{j > i} {\\langle v_i, v_j \\rangle x_i x_j}}$$\n",
    "\n",
    "where $\\hat{r}$ is the movie rating, $x_i$ are the one-hot encoded user and movie indicators, $w_i$ are linear weights, and the second term represents the pairwise feature interactions as a reduced dimension factorization.  This reduction in dimensionality allows us to find a smaller, more generalizable representation of information in the large sparse input dataset.  Key hyperparameters are therefore:\n",
    "\n",
    "- `feature_dim`: How big our initial movie \n",
    "- `num_factors`: How large our reduced dimensionality representation of interactions should be\n",
    "\n",
    "We also specify hyperparameters like `mini_batch_size` and `epochs` which can be tuned for improved performance.\n",
    "\n",
    "Hit \"play\" and sit back to let Sagemaker do all the work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fm_trained_model_location  = 's3://{}/{}/{}/output'.format(default_bucket, prefix, \"fm\")\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    train_instance_count=train_instance_count, \n",
    "    train_instance_type=train_instance_type,\n",
    "    output_path=fm_trained_model_location,\n",
    "    sagemaker_session=sagemaker.Session())\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=nbFeatures,\n",
    "    predictor_type='binary_classifier',\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=64,\n",
    "    epochs=3)\n",
    "\n",
    "fm.fit({'train': train_data_location, 'test': test_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model\n",
    "Now our trained model is stored, and you can actually see this in the Sagmaker console. We can go ahead and deploy this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fm_predictor = fm.deploy(instance_type='ml.c5.2xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make requests to our endpoint for prediction. Our data is really sparse (only 2 values in each vector row of 165237), and it doesn't make sense to send matrices that are mostly empty. To solve this problem, we are going to go ahead and create a protobuf serializer to send content in a sparse format is supported by sagemaker.\n",
    "\n",
    "You can see more formats here: https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_protobuf_serializer(data):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, data)\n",
    "    buf.seek(0)\n",
    "    return buf\n",
    "\n",
    "fm_predictor.content_type = 'application/x-recordio-protobuf'\n",
    "fm_predictor.serializer = sparse_protobuf_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions\n",
    "We made a dataset to run prediction earlier in this notebook. Let's go ahead and load that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred, Y_pred = loadDataset('ratings.shuffled.prediction', lines_prediction , nbFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab single row and run prediction on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_result = fm_predictor.predict(X_pred[:1])\n",
    "print(fm_result)\n",
    "true_labels = Y_pred[:1]\n",
    "\n",
    "for l,r in zip(true_labels, fm_result['predictions']):\n",
    "    print(\"TrueLabel:{} PredictedLabel:{} Score:{}\".format(l, int(r['predicted_label']), r['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of training job's accuracy\n",
    "Now that we've sanity checked that, let's just run inference on the entire prediction dataset of 2 million rows. Our feature dimension is wide but our data is mostly sparse, so we should be able to do relatively big batches of inferences. (This will take a couple of minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = lines_prediction\n",
    "predictions = []\n",
    "\n",
    "batches = int(X_pred.shape[0]/batch_size)\n",
    "\n",
    "for i in range(batches):\n",
    "    X_sample = X_pred[ (i*batch_size) : ((i+1)*batch_size) ]\n",
    "    result = fm_predictor.predict(X_sample)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_result = fm_predictor.predict(X_pred[1000:1003])\n",
    "print(fm_result)\n",
    "print(Y_pred[1000:1003])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what percentage we recommended correctly. This should be roughly equivalent to our testing metric that was reported during training, but provides independent validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]\n",
    "np.sum(Y_pred == predictions) / len(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a substantial improvement over a naive baseline ... add more here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up, and finish!\n",
    "And we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
